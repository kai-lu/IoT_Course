<!DOCTYPE html>
<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<title>Zotero Report</title>
		<link rel="stylesheet" type="text/css" href="data:text/css;base64,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"/>
		<link rel="stylesheet" type="text/css" media="screen,projection" href="data:text/css;base64,LyogR2VuZXJpYyBzdHlsZXMgKi8KYm9keSB7Cglmb250OiA2Mi41JSBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cgl3aWR0aDogNzgwcHg7CgltYXJnaW46IDAgYXV0bzsKfQoKaDIgewoJZm9udC1zaXplOiAxLjVlbTsKCWxpbmUtaGVpZ2h0OiAxLjVlbTsKCWZvbnQtZmFtaWx5OiBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cn0KCnAgewoJbGluZS1oZWlnaHQ6IDEuNWVtOwp9CgphOmxpbmssIGE6dmlzaXRlZCB7Cgljb2xvcjogIzkwMDsKfQoKYTpob3ZlciwgYTphY3RpdmUgewoJY29sb3I6ICM3Nzc7Cn0KCgp1bC5yZXBvcnQgewoJZm9udC1zaXplOiAxLjRlbTsKCXdpZHRoOiA2ODBweDsKCW1hcmdpbjogMCBhdXRvOwoJcGFkZGluZzogMjBweCAyMHB4Owp9CgovKiBNZXRhZGF0YSB0YWJsZSAqLwp0YWJsZSB7Cglib3JkZXI6IDFweCAjY2NjIHNvbGlkOwoJb3ZlcmZsb3c6IGF1dG87Cgl3aWR0aDogMTAwJTsKCW1hcmdpbjogLjFlbSBhdXRvIC43NWVtOwoJcGFkZGluZzogMC41ZW07Cn0K"/>
		<link rel="stylesheet" type="text/css" media="print" href="data:text/css;base64,Ym9keSB7Cglmb250OiAxMnB0ICJUaW1lcyBOZXcgUm9tYW4iLCBUaW1lcywgR2VvcmdpYSwgc2VyaWY7CgltYXJnaW46IDA7Cgl3aWR0aDogYXV0bzsKCWNvbG9yOiBibGFjazsKfQoKLyogUGFnZSBCcmVha3MgKHBhZ2UtYnJlYWstaW5zaWRlIG9ubHkgcmVjb2duaXplZCBieSBPcGVyYSkgKi8KaDEsIGgyLCBoMywgaDQsIGg1LCBoNiB7CglwYWdlLWJyZWFrLWFmdGVyOiBhdm9pZDsKCXBhZ2UtYnJlYWstaW5zaWRlOiBhdm9pZDsKfQoKdWwsIG9sLCBkbCB7CglwYWdlLWJyZWFrLWluc2lkZTogYXZvaWQ7Cgljb2xvci1hZGp1c3Q6IGV4YWN0Owp9CgpoMiB7Cglmb250LXNpemU6IDEuM2VtOwoJbGluZS1oZWlnaHQ6IDEuM2VtOwp9CgphIHsKCWNvbG9yOiAjMDAwOwoJdGV4dC1kZWNvcmF0aW9uOiBub25lOwp9Cg=="/>
	</head>
	<body>
		<ul class="report combineChildItems">
			<li id="item_HI9T3E55" class="item journalArticle">
			<h2>Multimedia traffic security architecture for the internet of things</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>L. Zhou</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>H. Chao</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>An important challenge for supporting multimedia applications in the Internet of Things is the security heterogeneity of wired and wireless sensor and actuator networks. In this work, we design a new and efficient media-aware security framework for facilitating various multimedia applications in the Internet of Things. First, we present a novel multimedia traffic classification and analysis method for handling the heterogeneity of diverse applications. Then a media-aware traffic security architecture is proposed based on the given traffic classification to enable various multimedia services being available anywhere and anytime. Furthermore, we provide a design rule and strategy to achieve a good trade-off between a system¿s flexibility and efficiency. To the best of our knowledge, this study is the first to provide general media-aware security architecture by jointly considering the characteristics of multimedia traffic, security service, and the Internet of Things.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>May 2011</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>IEEE Xplore</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Conference Name: IEEE Network</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>25</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>35-40</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>IEEE Network</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/MNET.2011.5772059">10.1109/MNET.2011.5772059</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>3</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1558-156X</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/6/2021, 7:07:32 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/6/2021, 7:07:32 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Authentication</li>
					<li>Cryptography</li>
					<li>Multimedia communication</li>
					<li>Radiofrequency identification</li>
					<li>Servers</li>
					<li>Streaming media</li>
					<li>Telecommunication traffic</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_U7K7ZC7G">zhou&amp;chao_2011_multimedia traffic security architecture for the internet of things.pdf					</li>
					<li id="item_PHEHU76Z">IEEE Xplore Abstract Record					</li>
				</ul>
			</li>


			<li id="item_UBRCHCQX" class="item conferencePaper">
			<h2>Large-Scale Video Classification with Convolutional Neural Networks</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Conference Paper</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>A. Karpathy</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>G. Toderici</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>S. Shetty</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>T. Leung</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>R. Sukthankar</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>L. Fei-Fei</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on large-scale video classification using a new dataset of 1 million YouTube videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display significant performance improvements compared to strong feature-based baselines (55.3% to 63.9%), but only a surprisingly modest improvement compared to single-frame models (59.3% to 60.9%). We further study the generalization performance of our best model by retraining the top layers on the UCF-101 Action Recognition dataset and observe significant performance improvements compared to the UCF-101 baseline model (63.3% up from 43.9%).</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>June 2014</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>IEEE Xplore</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>ISSN: 1063-6919</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1725-1732</td>
					</tr>
					<tr>
					<th>Proceedings Title</th>
						<td>2014 IEEE Conference on Computer Vision and Pattern Recognition</td>
					</tr>
					<tr>
					<th>Conference Name</th>
						<td>2014 IEEE Conference on Computer Vision and Pattern Recognition</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/CVPR.2014.223">10.1109/CVPR.2014.223</a></td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/6/2021, 7:06:56 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/6/2021, 7:06:56 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>action</li>
					<li>classification</li>
					<li>Computational modeling</li>
					<li>Computer architecture</li>
					<li>convolutional</li>
					<li>dataset</li>
					<li>Feature extraction</li>
					<li>large-scale</li>
					<li>network</li>
					<li>neural</li>
					<li>recognition</li>
					<li>Spatial resolution</li>
					<li>sports</li>
					<li>Streaming media</li>
					<li>Training</li>
					<li>video</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_MZICNUIG">karpathy et al_2014_large-scale video classification with convolutional neural networks.pdf					</li>
				</ul>
			</li>


			<li id="item_X7F9PUI3" class="item journalArticle">
			<h2>Face recognition: A literature survey</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>W. Zhao</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>R. Chellappa</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>P. J. Phillips</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>A. Rosenfeld</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>As one of the most successful applications of image analysis and understanding, face recognition has recently received significant attention, especially during the past several years. At least two reasons account for this trend: the first is the wide range of commercial and law enforcement applications, and the second is the availability of feasible technologies after 30 years of research. Even though current machine recognition systems have reached a certain level of maturity, their success is limited by the conditions imposed by many real applications. For example, recognition of face images acquired in an outdoor environment with changes in illumination and/or pose remains a largely unsolved problem. In other words, current systems are still far away from the capability of the human perception system.This paper provides an up-to-date critical survey of still- and video-based face recognition research. There are two underlying motivations for us to write this survey paper: the first is to provide an up-to-date review of the existing literature, and the second is to offer some insights into the studies of machine recognition of faces. To provide a comprehensive survey, we not only categorize existing recognition techniques but also present detailed descriptions of representative methods within each category. In addition, relevant topics such as psychophysical studies, system evaluation, and issues of illumination and pose variation are covered.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>December 1, 2003</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Face recognition</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>December 2003</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1145/954339.954342">https://doi.org/10.1145/954339.954342</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/6/2021, 7:06:13 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>35</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>399–458</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>ACM Computing Surveys</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1145/954339.954342">10.1145/954339.954342</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>4</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>ACM Comput. Surv.</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0360-0300</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/6/2021, 7:06:13 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/6/2021, 7:06:13 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Face recognition</li>
					<li>person identification</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_I7YUEEHK">zhao et al_2003_face recognition.pdf					</li>
				</ul>
			</li>


			<li id="item_8GY7XZC6" class="item journalArticle">
			<h2>Pattern Recognition and Machine Learning</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Christopher Bishop</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This leading textbook provides a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. This is the first machine learning textbook to include a comprehensive […]</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2006/01/01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en-US</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>www.microsoft.com</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.microsoft.com/en-us/research/publication/pattern-recognition-machine-learning/">https://www.microsoft.com/en-us/research/publication/pattern-recognition-machine-learning/</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/6/2021, 7:05:16 PM</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/6/2021, 7:05:16 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/6/2021, 7:05:16 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_JKS4UABP">bishop_2006_pattern recognition and machine learning.pdf					</li>
				</ul>
			</li>


			<li id="item_A6RB5JIU" class="item journalArticle">
			<h2>Assistive tagging: A survey of multimedia tagging with human-computer joint exploration</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Meng Wang</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Bingbing Ni</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Xian-Sheng Hua</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Tat-Seng Chua</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>Along with the explosive growth of multimedia data, automatic multimedia tagging has attracted great interest of various research communities, such as computer vision, multimedia, and information retrieval. However, despite the great progress achieved in the past two decades, automatic tagging technologies still can hardly achieve satisfactory performance on real-world multimedia data that vary widely in genre, quality, and content. Meanwhile, the power of human intelligence has been fully demonstrated in the Web 2.0 era. If well motivated, Internet users are able to tag a large amount of multimedia data. Therefore, a set of new techniques has been developed by combining humans and computers for more accurate and efficient multimedia tagging, such as batch tagging, active tagging, tag recommendation, and tag refinement. These techniques are able to accomplish multimedia tagging by jointly exploring humans and computers in different ways. This article refers to them collectively as assistive tagging and conducts a comprehensive survey of existing research efforts on this theme. We first introduce the status of automatic tagging and manual tagging and then state why assistive tagging can be a good solution. We categorize existing assistive tagging techniques into three paradigms: (1) tagging with data selection &amp; organization; (2) tag recommendation; and (3) tag processing. We introduce the research efforts on each paradigm and summarize the methodologies. We also provide a discussion on several future trends in this research direction.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>September 7, 2012</td>
					</tr>
					<tr>
					<th>Short Title</th>
						<td>Assistive tagging</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>August 2012</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://doi.org/10.1145/2333112.2333120">https://doi.org/10.1145/2333112.2333120</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/6/2021, 7:04:34 PM</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>44</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>25:1–25:24</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>ACM Computing Surveys</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1145/2333112.2333120">10.1145/2333112.2333120</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>4</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>ACM Comput. Surv.</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0360-0300</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/6/2021, 7:04:34 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/6/2021, 7:04:34 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Annotation</li>
					<li>interactive tagging</li>
					<li>tag location</li>
					<li>tag recommendation</li>
					<li>tag refinement</li>
					<li>tagging</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_V5BEN545">wang et al_2012_assistive tagging.pdf					</li>
				</ul>
			</li>


			<li id="item_7QW6SGK4" class="item journalArticle">
			<h2>Affective video content representation and modeling</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>A. Hanjalic</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>Li-Qun Xu</td>
					</tr>
					<tr>
					<th>Abstract</th>
						<td>This paper looks into a new direction in video content analysis - the representation and modeling of affective video content . The affective content of a given video clip can be defined as the intensity and type of feeling or emotion (both are referred to as affect) that are expected to arise in the user while watching that clip. The availability of methodologies for automatically extracting this type of video content will extend the current scope of possibilities for video indexing and retrieval. For instance, we will be able to search for the funniest or the most thrilling parts of a movie, or the most exciting events of a sport program. Furthermore, as the user may want to select a movie not only based on its genre, cast, director and story content, but also on its prevailing mood, the affective content analysis is also likely to contribute to enhancing the quality of personalizing the video delivery to the user. We propose in this paper a computational framework for affective video content representation and modeling. This framework is based on the dimensional approach to affect that is known from the field of psychophysiology. According to this approach, the affective video content can be represented as a set of points in the two-dimensional (2-D) emotion space that is characterized by the dimensions of arousal (intensity of affect) and valence (type of affect). We map the affective video content onto the 2-D emotion space by using the models that link the arousal and valence dimensions to low-level features extracted from video data. This results in the arousal and valence time curves that, either considered separately or combined into the so-called affect curve, are introduced as reliable representations of expected transitions from one feeling to another along a video, as perceived by a viewer.</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>February 2005</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>IEEE Xplore</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Conference Name: IEEE Transactions on Multimedia</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>7</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>143-154</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>IEEE Transactions on Multimedia</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1109/TMM.2004.840618">10.1109/TMM.2004.840618</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>1</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1941-0077</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/6/2021, 7:03:50 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/6/2021, 7:03:50 PM</td>
					</tr>
				</table>
				<h3 class="tags">Tags:</h3>
				<ul class="tags">
					<li>Affective video content analysis</li>
					<li>Algorithm design and analysis</li>
					<li>Content based retrieval</li>
					<li>Data mining</li>
					<li>Indexing</li>
					<li>Information filtering</li>
					<li>Information filters</li>
					<li>Layout</li>
					<li>Mood</li>
					<li>Motion pictures</li>
					<li>Streaming media</li>
					<li>video abstraction</li>
					<li>video content modeling</li>
					<li>video content representation</li>
					<li>video highlights extraction</li>
				</ul>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_CPYJDV7P">IEEE Xplore Abstract Record					</li>
					<li id="item_84EEET2W">hanjalic&amp;li-qun xu_2005_affective video content representation and modeling.pdf					</li>
				</ul>
			</li>


			<li id="item_2AZVUUFW" class="item journalArticle">
			<h2>A Survey of Augmented Reality Technologies, Applications and Limitations</h2>
				<table>
					<tr>
						<th>Type</th>
						<td>Journal Article</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>D. W. F. van Krevelen</td>
					</tr>
					<tr>
						<th class="author">Author</th>
						<td>R. Poelman</td>
					</tr>
					<tr>
					<th>Date</th>
						<td>2010/01/01</td>
					</tr>
					<tr>
					<th>Language</th>
						<td>en</td>
					</tr>
					<tr>
					<th>Library Catalog</th>
						<td>ijvr.eu</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://ijvr.eu/article/view/2767">https://ijvr.eu/article/view/2767</a></td>
					</tr>
					<tr>
					<th>Accessed</th>
						<td>4/6/2021, 7:03:19 PM</td>
					</tr>
					<tr>
					<th>Rights</th>
						<td>Copyright (c) 2010 International Journal of Virtual Reality</td>
					</tr>
					<tr>
					<th>Extra</th>
						<td>Number: 2</td>
					</tr>
					<tr>
					<th>Volume</th>
						<td>9</td>
					</tr>
					<tr>
					<th>Pages</th>
						<td>1-20</td>
					</tr>
					<tr>
					<th>Publication</th>
						<td>International Journal of Virtual Reality</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.20870/IJVR.2010.9.2.2767">10.20870/IJVR.2010.9.2.2767</a></td>
					</tr>
					<tr>
					<th>Issue</th>
						<td>2</td>
					</tr>
					<tr>
					<th>Journal Abbr</th>
						<td>IJVR</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>1081-1451</td>
					</tr>
					<tr>
					<th>Date Added</th>
						<td>4/6/2021, 7:03:19 PM</td>
					</tr>
					<tr>
					<th>Modified</th>
						<td>4/6/2021, 7:03:19 PM</td>
					</tr>
				</table>
				<h3 class="attachments">Attachments</h3>
				<ul class="attachments">
					<li id="item_MPHFFQ9J">krevelen&amp;poelman_2010_a survey of augmented reality technologies, applications and limitations.pdf					</li>
					<li id="item_AWSF9DD6">Snapshot					</li>
				</ul>
			</li>

		</ul>
	</body>
</html>